{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=False, transform=transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='../data', train=False, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(mnist_trainset,batch_size=64,shuffle=True)\n",
    "test_dataloader = DataLoader(mnist_testset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=6,\n",
    "            kernel_size=(5, 5),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=6,\n",
    "            out_channels=16,\n",
    "            kernel_size=(5, 5),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=120,\n",
    "            kernel_size=(4, 4),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "        )\n",
    "        self.linear1 = nn.Linear(120, 84)\n",
    "        self.linear2 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x)) #24d\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(\n",
    "            self.conv3(x)\n",
    "        )  # num_examples x 120 x 1 x 1 --> num_examples x 120\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(mnist_trainset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (relu): ReLU()\n",
      "  (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(16, 120, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (linear1): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (linear2): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,criterion,train_dataloader,device,EPOCHS,evaluate_epoch=True):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    total = len(train_dataloader) * EPOCHS\n",
    "    log = defaultdict(list)\n",
    "    log[\"train_loss\"] = []\n",
    "    loss_a = []\n",
    "    \n",
    "    with tqdm(total=total,desc=\"Training\") as tt:\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            \n",
    "            total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "            \n",
    "            for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "                \n",
    "                model.train()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                batch_counts +=1\n",
    "                \n",
    "                out = model(data)\n",
    "                \n",
    "                loss = criterion(out, target)\n",
    "                            \n",
    "                loss.backward()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                loss_a.append(loss.item())\n",
    "                \n",
    "            if evaluate_epoch:\n",
    "                print(f\"{epoch+1}/{EPOCHS}:\")\n",
    "                print(f\"  - Train Loss: {np.mean(loss_a)}\")\n",
    "                log[\"train_loss\"].append(total_loss / len(train_dataloader))\n",
    "            tt.update()\n",
    "            \n",
    "        print(\"Done.\")\n",
    "        tt.close()\n",
    "    return model, log\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,criterion,test_dataloader,device):\n",
    "    \n",
    "    total = len(test_dataloader)\n",
    "    loss_a = []\n",
    "    \n",
    "        \n",
    "    with tqdm(total=total,leave=False,desc=\"Testing\") as ee:\n",
    "            \n",
    "        test_loss, test_batch_loss, test_batch_counts = 0, 0, 0\n",
    "        model.eval()\n",
    "        for data, target in test_dataloader:\n",
    "                \n",
    "            test_batch_counts += 1\n",
    "            with torch.no_grad():\n",
    "                out = model.forward(data)\n",
    "            \n",
    "            loss = criterion(out,target)\n",
    "            loss_a.append(loss.detach().cpu().numpy())\n",
    "            test_batch_loss += loss.item()\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "        ee.update()\n",
    "        \n",
    "        print(f\"  - Test Loss: {np.mean(test_loss)}\")\n",
    "            \n",
    "        \n",
    "            \n",
    "        ee.close()\n",
    "        \n",
    "    return np.mean(loss_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "EPOCHS = 20\n",
    "opts = {\n",
    "    'lr': 3e-4,\n",
    "    'batch_size': 64\n",
    "}\n",
    "optimizer = torch.optim.Adam(lenet.parameters(), opts['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4be771c9471481da529eef2b1902a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/18760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20:\n",
      "  - Train Loss: 0.07982615072693207\n",
      "2/20:\n",
      "  - Train Loss: 0.0759800370839208\n",
      "3/20:\n",
      "  - Train Loss: 0.07241518785693837\n",
      "4/20:\n",
      "  - Train Loss: 0.06940734456653204\n",
      "5/20:\n",
      "  - Train Loss: 0.06673304888539945\n",
      "6/20:\n",
      "  - Train Loss: 0.06406645385336492\n",
      "7/20:\n",
      "  - Train Loss: 0.06159923783106817\n",
      "8/20:\n",
      "  - Train Loss: 0.05945473351845001\n",
      "9/20:\n",
      "  - Train Loss: 0.057381606123593186\n",
      "10/20:\n",
      "  - Train Loss: 0.055399234480110866\n",
      "11/20:\n",
      "  - Train Loss: 0.05354691390084651\n",
      "12/20:\n",
      "  - Train Loss: 0.05183530277070805\n",
      "13/20:\n",
      "  - Train Loss: 0.05014719132110968\n",
      "14/20:\n",
      "  - Train Loss: 0.04861726811115261\n",
      "15/20:\n",
      "  - Train Loss: 0.04711127359275762\n",
      "16/20:\n",
      "  - Train Loss: 0.04569027713291776\n",
      "17/20:\n",
      "  - Train Loss: 0.0443157401720603\n",
      "18/20:\n",
      "  - Train Loss: 0.043047751038963016\n",
      "19/20:\n",
      "  - Train Loss: 0.04180576612045598\n",
      "20/20:\n",
      "  - Train Loss: 0.040687713570008326\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LeNet(\n",
       "   (relu): ReLU()\n",
       "   (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
       "   (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "   (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "   (conv3): Conv2d(16, 120, kernel_size=(4, 4), stride=(1, 1))\n",
       "   (linear1): Linear(in_features=120, out_features=84, bias=True)\n",
       "   (linear2): Linear(in_features=84, out_features=10, bias=True)\n",
       " ),\n",
       " defaultdict(list,\n",
       "             {'train_loss': [0.07982615072693207,\n",
       "               0.07213392344090952,\n",
       "               0.06528548940297352,\n",
       "               0.060383814695313065,\n",
       "               0.056035866160869106,\n",
       "               0.05073347869319226,\n",
       "               0.04679594169728764,\n",
       "               0.04444320333012288,\n",
       "               0.04079658696473862,\n",
       "               0.03755788968876997,\n",
       "               0.03502370810820294,\n",
       "               0.033007580339185036,\n",
       "               0.02988985392592921,\n",
       "               0.028728266381710738,\n",
       "               0.02602735033522778,\n",
       "               0.024375330235319844,\n",
       "               0.02232314879834092,\n",
       "               0.021491935776309173,\n",
       "               0.019450037587329303,\n",
       "               0.019444715111502888]}))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model=lenet,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_dataloader=train_dataloader,\n",
    "    device=device,\n",
    "    EPOCHS=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784d72c0a2104268bdcfea9b795377e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Test Loss: 8.235335960547673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.052454364"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model=lenet,\n",
    "    criterion=criterion,\n",
    "    test_dataloader=test_dataloader,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9843\n",
      "157\n",
      "98.43\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    true = 0\n",
    "    false = 0\n",
    "    for index in range(len(mnist_testset)):\n",
    "        \n",
    "        item = mnist_testset[index]\n",
    "        image=item[0].reshape([1,1,28,28])\n",
    "        true_target = item[1]\n",
    "        \n",
    "        pred = lenet(image)\n",
    "        predicted_class = np.argmax(pred)\n",
    "        \n",
    "        if predicted_class == true_target:\n",
    "            true +=1\n",
    "        else:\n",
    "            false +=1\n",
    "            \n",
    "    accuracy = (100 * true)/len(mnist_testset)\n",
    "    \n",
    "print(true)\n",
    "print(false)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    index = 4\n",
    "    \n",
    "    \n",
    "    item = mnist_testset[index]\n",
    "    image=item[0].reshape([1,1,28,28])\n",
    "    true_target = item[1]\n",
    "    \n",
    "    \n",
    "    prediction = lenet(image)\n",
    "    \n",
    "    predicted_class = np.argmax(prediction)\n",
    "    \n",
    "    image = image.reshape(28, 28, 1)\n",
    "    \n",
    "    # Show result\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f'Prediction: {predicted_class} - Actual target: {true_target}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lenet, f\"lenet_{accuracy}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (relu): ReLU()\n",
       "  (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(16, 120, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (linear1): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (linear2): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(f\"lenet_{accuracy}.pt\")\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
